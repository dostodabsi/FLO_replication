\documentclass{beamer}            
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{textcomp}
\usepackage{epstopdf,tikz}
\usepackage{amsmath}

\usepackage[backend=biber, style=apa]{biblatex}
\DeclareLanguageMapping{american}{american-apa}

\usefonttheme[onlymath]{serif}

%%% defines highlight command to set text blue
\newcommand{\highlight}[1]{{\color{blue}{#1}}}
\addbibresource{bibliography.bib}

\usetheme{Boadilla}

\title{Feature-Label-Ordering}
\subtitle{a pre-registered Bayesian replication using MTurk}
\date{\today}
\author{Fabian Dablander \& Marcel Bechtold}
\institute{University of T\"{u}bingen}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
	\begin{quote}
		Gregory: ``Is there any other point to which you would wish to draw my attention?''\\
		Holmes: ``To the curious incident of the dog in the nighttime.''\\
		Gregoy: ``The dog did nothing in the nighttime.''\\
		Holmes: ``That was the curious incident.''\\
	\end{quote}
\end{frame}

\begin{frame}
	\begin{center}
		\includegraphics[scale=0.7]{figure/meme.png}
	\end{center}
\end{frame}

\section{Introduction}
\begin{frame}{Error-driven learning}
    \begin{center}
        $V^{t+1}_{ij} = V^{t}_{ij} + \bigtriangleup V^t_{ij}$
    \end{center}
    
    \begin{small}
      \begin{equation*}
          \bigtriangleup V^t_{ij} = \begin{cases}
              0, & \text{if ABSENT($C_i$, t)} \\
              \alpha_i \beta_1(\lambda - \sum\nolimits_{\text{present{($C_j$, t)}}} V_{ij}), & \text{if PRESENT($C_j$, t) \& PRESENT(0, t)}\\
              \alpha_i \beta_2(0 - \sum\nolimits_{\text{present{($C_j$, t)}}} V_{ij}), & \text{if PRESENT($C_j$, t) \& ABSENT(0, t)}\\
             \end{cases}
      \end{equation*}
    \end{small}
\end{frame}

\begin{frame}{FL-learning vs. LF-learning\footnote{all images from Ramscar et al. (2010)}}
    \begin{center}
		\includegraphics[scale=.4]{figure/FL_learning.png}
	\end{center}
\end{frame}

\begin{frame}{FL-learning vs. LF-learning}
    \begin{center}
    	\includegraphics[scale=.4]{figure/LF_learning.png}
    \end{center}
\end{frame}

\begin{frame}{No representation without taxation (\cite{ramscar2009no})}
    \begin{center}
    	\includegraphics[scale=.4]{figure/representation.png}
    \end{center}
\end{frame}

\section{Design}
\begin{frame}{Example Trial\footnote{\href{http://wiki.cnbc.cmu.edu/Image_Databases}{great image databases: here}}}
    \begin{center}
        \includegraphics[scale=.45]{figure/trial.png}
	\end{center}
\end{frame}

\begin{frame}{Their Results}
    \begin{itemize}[<+->]
        \item All subjects were tested on a category verification and recognition task.
        \item A 2(learning) x 2(task) ANOVA found that training using FL-examples lead to better
              performance on the category verification task.
        \item Conversely, when trained on LF-examples, they scored higher on the recognition task.
        
        \item shows that \textbf{improved response-discrimination leads to the original input being less accurately represented}
    \end{itemize}
\end{frame}

\begin{frame}{Our Goal}
We want to replicate those results using an Amazon Mechanical Turk sample and using different statistical techniques.
\end{frame}

\section{Analysis}
\begin{frame}[fragile]{Bayesian Signal Detection Theory}
<<bayesianSDT, message=FALSE>>=
library('rjags')

ms <- '
model {
  hits ~ dbin(theta_h, signal)
  falarms ~ dbin(theta_f, noise)
    
  theta_h <- phi(d/2-c)
  theta_f <- phi(-d/2-c)
    
  d ~ dnorm(0, .5)
  c ~ dnorm(0, 2)
}'

# see also Lee & Wagenmakers (2013, pp.156)
@
\end{frame}

\begin{frame}[fragile]{Example}
<<example, message=FALSE>>=
library('ggmcmc')

params <- c('d', 'c', 'theta_h', 'theta_f')
data <- list('hits' = 70, 'falarms' = 50,
             'signal' = 70 + 30, 'noise' = 50 + 50)
             
model <- jags.model(textConnection(ms), data = data,
                    n.chains = 2, quiet = TRUE)
                    
samples <- coda.samples(model, n.iter = 10000,
                        variable.names = params)
@
\end{frame}

\begin{frame}[fragile]{Example}
<<plot, message=FALSE, fig.width=5.5, fig.height=3.2>>=
ggs_density(ggs(samples[, 1:2]))
@
\end{frame}

\begin{frame}[fragile]{Mixed Logit Models}
Following \textcite{jaeger2008categorical}, \textcite{baayen2008mixed}, and \textcite{judd2012treating}, we want to analyze the gathered data
using a Mixed Logit Model with \emph{participant} and \emph{stimulus} as random factors.

<<example_data, echo=FALSE>>=
data <- rbind(c(1, 1, 0, 0, 'wug', 'FL', 'recognition'),
              c(2, 0, 1, 0, 'niz', 'FL', 'verification'),
              c(3, 1, 0, 0, 'mob', 'LF', 'recognition'),
              c(4, 0, 0, 1, 'mob', 'FL', 'recognition'),
              
              c(5, 0, 0, 1, 'miz', 'FL', 'verification'),
              c(6, 0, 1, 0, 'miz', 'LF', 'verification'),
              c(7, 1, 1, 1, 'niz', 'LF', 'recognition'),
              c(8, 1, 1, 1, 'wug', 'LF', 'verification'))
              
data <- data.frame(data)
names(data) <- c('id', 'resp', 'present', 'cor', 'alien', 'learning', 'task')
add <- function(data, times) {
    res <- data
    for (i in 1:times) {
        res <- rbind(res, res[floor(runif(1, 1, nrow(res))), ])
    }
    res
}

data <- add(data, 100)
data$cor <- as.numeric(data$cor) - 1
data$id <- as.factor(1:nrow(data))
@


<<lmm, message=FALSE>>=
library('lme4')

# example data
head(data, 3)

# example model
fit <- glmer(cor ~ learning*task + (1|id) + (1|alien),
                   data, binomial)
@
\end{frame}

\section{Conclusion}
\begin{frame}{Plan}
    \begin{itemize}[<+->]
        \item pre-register replication on the \href{http://osf.io}{Open Science Framework}
        \item implement experiment in JavaScript, run it on Mechanical Turk
        \item implement Rescorla-Wagner simulations in R (Julia?)
        \item analyze data using Bayesian SDT and Mixed Logit Models
    \end{itemize}
\end{frame}

\end{document}